#!/usr/bin/env python3
"""
ì›¹ ë¡œê·¸ ë°ì´í„° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ weblog.csv ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ COSMOS AIOps ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime
from collections import Counter
import sys
import os

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def analyze_weblog_structure():
    """ì›¹ ë¡œê·¸ ë°ì´í„° êµ¬ì¡° ë¶„ì„"""
    print("ğŸ“Š ì›¹ ë¡œê·¸ ë°ì´í„° êµ¬ì¡° ë¶„ì„")
    print("=" * 50)
    
    try:
        # CSV íŒŒì¼ ì½ê¸°
        df = pd.read_csv('weblog.csv')
        
        print(f"ğŸ“ˆ ê¸°ë³¸ ì •ë³´:")
        print(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {len(df):,}ê°œ")
        print(f"   ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ")
        print(f"   ì»¬ëŸ¼ëª…: {list(df.columns)}")
        
        print(f"\nğŸ“‹ ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ):")
        print(df.head())
        
        print(f"\nğŸ“Š ë°ì´í„° íƒ€ì…:")
        print(df.dtypes)
        
        print(f"\nğŸ” ê²°ì¸¡ê°’ ì •ë³´:")
        print(df.isnull().sum())
        
        return df
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì½ê¸° ì‹¤íŒ¨: {e}")
        return None

def analyze_ip_patterns(df):
    """IP íŒ¨í„´ ë¶„ì„"""
    print(f"\nğŸŒ IP íŒ¨í„´ ë¶„ì„")
    print("-" * 30)
    
    # IP ì£¼ì†Œë³„ ìš”ì²­ ìˆ˜
    ip_counts = df['IP'].value_counts()
    
    print(f"ğŸ“Š IP í†µê³„:")
    print(f"   ê³ ìœ  IP ìˆ˜: {len(ip_counts)}ê°œ")
    print(f"   ìƒìœ„ 5ê°œ IP:")
    for ip, count in ip_counts.head().items():
        print(f"     {ip}: {count:,}íšŒ ìš”ì²­")
    
    # IP ë²”ìœ„ ë¶„ì„
    ip_ranges = df['IP'].str.extract(r'(\d+\.\d+\.\d+)\.\d+')[0].value_counts()
    print(f"\nğŸ“ˆ IP ëŒ€ì—­ë³„ ìš”ì²­:")
    for ip_range, count in ip_ranges.head().items():
        print(f"     {ip_range}.x: {count:,}íšŒ ìš”ì²­")
    
    return ip_counts

def analyze_url_patterns(df):
    """URL íŒ¨í„´ ë¶„ì„"""
    print(f"\nğŸ”— URL íŒ¨í„´ ë¶„ì„")
    print("-" * 30)
    
    # URLë³„ ìš”ì²­ ìˆ˜
    url_counts = df['URL'].value_counts()
    
    print(f"ğŸ“Š URL í†µê³„:")
    print(f"   ê³ ìœ  URL ìˆ˜: {len(url_counts)}ê°œ")
    print(f"   ìƒìœ„ 10ê°œ URL:")
    for url, count in url_counts.head(10).items():
        # URLì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
        display_url = url[:80] + "..." if len(url) > 80 else url
        print(f"     {display_url}: {count:,}íšŒ")
    
    # HTTP ë©”ì„œë“œ ë¶„ì„
    http_methods = df['URL'].str.extract(r'^([A-Z]+)')[0].value_counts()
    print(f"\nğŸ“ˆ HTTP ë©”ì„œë“œ ë¶„í¬:")
    for method, count in http_methods.items():
        print(f"     {method}: {count:,}íšŒ")
    
    # íŒŒì¼ í™•ì¥ì ë¶„ì„
    extensions = df['URL'].str.extract(r'\.([a-zA-Z0-9]+)')[0].value_counts()
    print(f"\nğŸ“ íŒŒì¼ í™•ì¥ì ë¶„í¬:")
    for ext, count in extensions.head(10).items():
        if pd.notna(ext):
            print(f"     .{ext}: {count:,}íšŒ")
    
    return url_counts

def analyze_status_codes(df):
    """ìƒíƒœ ì½”ë“œ ë¶„ì„"""
    print(f"\nğŸ“Š HTTP ìƒíƒœ ì½”ë“œ ë¶„ì„")
    print("-" * 30)
    
    status_counts = df['Staus'].value_counts().sort_index()
    
    print(f"ğŸ“ˆ ìƒíƒœ ì½”ë“œ ë¶„í¬:")
    for status, count in status_counts.items():
        percentage = (count / len(df)) * 100
        print(f"     {status}: {count:,}íšŒ ({percentage:.1f}%)")
    
    # ì—ëŸ¬ ìƒíƒœ ì½”ë“œ ë¶„ì„
    error_codes = [400, 401, 403, 404, 500, 502, 503]
    error_count = 0
    for code in error_codes:
        if code in status_counts:
            error_count += status_counts[code]
    
    error_percentage = (error_count / len(df)) * 100
    print(f"\nâš ï¸ ì—ëŸ¬ ìƒíƒœ ì½”ë“œ ì´í•©: {error_count:,}íšŒ ({error_percentage:.1f}%)")
    
    return status_counts

def analyze_time_patterns(df):
    """ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
    print(f"\nâ° ì‹œê°„ íŒ¨í„´ ë¶„ì„")
    print("-" * 30)
    
    # ì‹œê°„ íŒŒì‹± (ê°„ë‹¨í•œ ì˜ˆì‹œ)
    time_data = df['Time'].str.extract(r'\[(\d+/\w+/\d+):(\d+:\d+:\d+)')
    
    if not time_data.empty:
        time_data.columns = ['date', 'time']
        
        # ì‹œê°„ëŒ€ë³„ ë¶„ì„
        time_data['hour'] = time_data['time'].str.extract(r'(\d+):').astype(int)
        hourly_counts = time_data['hour'].value_counts().sort_index()
        
        print(f"ğŸ“ˆ ì‹œê°„ëŒ€ë³„ ìš”ì²­ ë¶„í¬:")
        for hour, count in hourly_counts.items():
            print(f"     {hour:02d}ì‹œ: {count:,}íšŒ")
        
        # ë‚ ì§œë³„ ë¶„ì„
        date_counts = time_data['date'].value_counts().sort_index()
        print(f"\nğŸ“… ë‚ ì§œë³„ ìš”ì²­ ë¶„í¬:")
        for date, count in date_counts.head(10).items():
            print(f"     {date}: {count:,}íšŒ")
    
    return time_data

def detect_anomalies(df):
    """ì´ìƒ íŒ¨í„´ íƒì§€"""
    print(f"\nğŸš¨ ì´ìƒ íŒ¨í„´ íƒì§€")
    print("-" * 30)
    
    anomalies = []
    
    # 1. ê³¼ë„í•œ ìš”ì²­ IP íƒì§€
    ip_counts = df['IP'].value_counts()
    ip_threshold = ip_counts.quantile(0.95)  # ìƒìœ„ 5% IP
    high_request_ips = ip_counts[ip_counts > ip_threshold]
    
    if len(high_request_ips) > 0:
        anomalies.append({
            'type': 'high_request_ip',
            'count': len(high_request_ips),
            'description': f'{ip_threshold:.0f}íšŒ ì´ìƒ ìš”ì²­í•œ IP {len(high_request_ips)}ê°œ ë°œê²¬'
        })
        print(f"âš ï¸ ê³¼ë„í•œ ìš”ì²­ IP: {len(high_request_ips)}ê°œ")
        for ip, count in high_request_ips.head(5).items():
            print(f"     {ip}: {count:,}íšŒ")
    
    # 2. ì—ëŸ¬ ìƒíƒœ ì½”ë“œ íƒì§€
    error_codes = [400, 401, 403, 404, 500, 502, 503]
    error_requests = df[df['Staus'].isin(error_codes)]
    error_rate = len(error_requests) / len(df) * 100
    
    if error_rate > 5:  # ì—ëŸ¬ìœ¨ 5% ì´ìƒ
        anomalies.append({
            'type': 'high_error_rate',
            'count': len(error_requests),
            'description': f'ì—ëŸ¬ìœ¨ {error_rate:.1f}% ({len(error_requests):,}ê°œ ì—ëŸ¬)'
        })
        print(f"âš ï¸ ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.1f}%")
    
    # 3. ë¹„ì •ìƒì ì¸ URL íŒ¨í„´ íƒì§€
    url_counts = df['URL'].value_counts()
    suspicious_urls = url_counts[url_counts > 1000]  # 1000íšŒ ì´ìƒ ìš”ì²­ëœ URL
    
    if len(suspicious_urls) > 0:
        anomalies.append({
            'type': 'suspicious_url',
            'count': len(suspicious_urls),
            'description': f'1000íšŒ ì´ìƒ ìš”ì²­ëœ URL {len(suspicious_urls)}ê°œ'
        })
        print(f"âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ URL: {len(suspicious_urls)}ê°œ")
    
    print(f"\nğŸ“Š íƒì§€ëœ ì´ìƒ íŒ¨í„´: {len(anomalies)}ê°œ")
    for anomaly in anomalies:
        print(f"   â€¢ {anomaly['description']}")
    
    return anomalies

def test_cosmos_aiops_with_real_data(df):
    """COSMOS AIOps ì‹œìŠ¤í…œìœ¼ë¡œ ì‹¤ì œ ë°ì´í„° í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ¤– COSMOS AIOps ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("-" * 40)
    
    try:
        from unified_cosmos import COSMOSFactory, DomainType
        
        # AIOps ì—”ì§„ ìƒì„±
        aiops_engine = COSMOSFactory.create_aiops_system()
        print("âœ… AIOps ì—”ì§„ ìƒì„± ì™„ë£Œ")
        
        # ì‹¤ì œ ë¡œê·¸ ë°ì´í„° ì¤€ë¹„
        sample_logs = df.head(100).to_dict('records')
        
        # COSMOS ì‹œìŠ¤í…œìœ¼ë¡œ ì²˜ë¦¬
        test_data = {
            'logs': sample_logs,
            'component': 'web-server',
            'total_requests': len(df),
            'error_rate': len(df[df['Staus'].isin([400, 401, 403, 404, 500, 502, 503])]) / len(df)
        }
        
        print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„:")
        print(f"   ë¡œê·¸ ìƒ˜í”Œ: {len(sample_logs)}ê°œ")
        print(f"   ì „ì²´ ìš”ì²­: {test_data['total_requests']:,}ê°œ")
        print(f"   ì—ëŸ¬ìœ¨: {test_data['error_rate']:.2%}")
        
        # COSMOS ì‹¤í–‰
        result = aiops_engine.execute_with_unified_duality(
            rule_sequence=['validate_input', 'parse_log_entries', 'detect_anomalies', 'generate_alerts'],
            input_data=test_data,
            domain_context=DomainType.AIOPS
        )
        
        print(f"\nğŸ¯ COSMOS ì‹¤í–‰ ê²°ê³¼:")
        print(f"   ëª¨ë“œ: {result['mode']}")
        print(f"   ë„ë©”ì¸: {result['domain']}")
        print(f"   ì˜í–¥ë„: {result['total_impact']:.3f}")
        print(f"   ì‹¤í–‰ ì‹œê°„: {result['total_duration_ms']:.1f}ms")
        print(f"   ì‹¤í–‰ ê²½ë¡œ: {len(result['execution_path'])}ë‹¨ê³„")
        
        print(f"\nğŸ“‹ ì‹¤í–‰ ê²½ë¡œ ìƒì„¸:")
        for i, step in enumerate(result['execution_path'], 1):
            print(f"   {i}. {step['rule']} [{step['layer']}] - {step['status']} (ì˜í–¥ë„: {step['impact']:.3f})")
        
        return result
        
    except Exception as e:
        print(f"âŒ COSMOS í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
        return None

def generate_report(df, anomalies, cosmos_result):
    """ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
    print(f"\nğŸ“„ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±")
    print("-" * 30)
    
    report = {
        'analysis_date': datetime.now().isoformat(),
        'dataset_info': {
            'total_records': len(df),
            'columns': list(df.columns),
            'date_range': '2017-11-29',
            'data_source': 'weblog.csv'
        },
        'statistics': {
            'unique_ips': len(df['IP'].unique()),
            'unique_urls': len(df['URL'].unique()),
            'error_rate': len(df[df['Staus'].isin([400, 401, 403, 404, 500, 502, 503])]) / len(df),
            'top_ip': df['IP'].value_counts().index[0],
            'most_common_status': df['Staus'].value_counts().index[0]
        },
        'anomalies_detected': len(anomalies),
        'anomaly_details': anomalies,
        'cosmos_test': {
            'success': cosmos_result is not None,
            'impact': cosmos_result['total_impact'] if cosmos_result else None,
            'execution_time_ms': cosmos_result['total_duration_ms'] if cosmos_result else None
        }
    }
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open('weblog_analysis_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"âœ… ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: weblog_analysis_report.json")
    
    return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì›¹ ë¡œê·¸ ë°ì´í„° ë¶„ì„ ë° COSMOS AIOps í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # 1. ë°ì´í„° êµ¬ì¡° ë¶„ì„
    df = analyze_weblog_structure()
    if df is None:
        return False
    
    # 2. IP íŒ¨í„´ ë¶„ì„
    ip_analysis = analyze_ip_patterns(df)
    
    # 3. URL íŒ¨í„´ ë¶„ì„
    url_analysis = analyze_url_patterns(df)
    
    # 4. ìƒíƒœ ì½”ë“œ ë¶„ì„
    status_analysis = analyze_status_codes(df)
    
    # 5. ì‹œê°„ íŒ¨í„´ ë¶„ì„
    time_analysis = analyze_time_patterns(df)
    
    # 6. ì´ìƒ íŒ¨í„´ íƒì§€
    anomalies = detect_anomalies(df)
    
    # 7. COSMOS AIOps ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    cosmos_result = test_cosmos_aiops_with_real_data(df)
    
    # 8. ë³´ê³ ì„œ ìƒì„±
    report = generate_report(df, anomalies, cosmos_result)
    
    print(f"\nğŸ‰ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì´ {len(df):,}ê°œ ë¡œê·¸ ë ˆì½”ë“œ ë¶„ì„")
    print(f"ğŸš¨ {len(anomalies)}ê°œ ì´ìƒ íŒ¨í„´ íƒì§€")
    print(f"ğŸ¤– COSMOS AIOps ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ {'ì„±ê³µ' if cosmos_result else 'ì‹¤íŒ¨'}")
    
    return True

if __name__ == "__main__":
    main()
