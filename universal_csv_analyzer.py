#!/usr/bin/env python3
"""
Universal CSV Analyzer for COSMOS AIOps
ëª¨ë“  CSV íŒŒì¼ ìœ í˜•ê³¼ ì–‘ì‹ì— ëŒ€ì‘í•˜ëŠ” ë²”ìš© ë¶„ì„ê¸°
"""

import pandas as pd
import numpy as np
import json
import csv
import chardet
import os
import sys
from datetime import datetime
from pathlib import Path
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

class UniversalCSVAnalyzer:
    """ëª¨ë“  CSV íŒŒì¼ ìœ í˜•ì„ ì§€ì›í•˜ëŠ” ë²”ìš© ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.supported_formats = {
            'web_logs': ['apache', 'nginx', 'iis', 'web', 'access', 'log'],
            'system_logs': ['syslog', 'system', 'kernel', 'audit'],
            'application_logs': ['app', 'application', 'service', 'daemon'],
            'network_logs': ['network', 'firewall', 'router', 'switch'],
            'security_logs': ['security', 'auth', 'login', 'audit'],
            'database_logs': ['database', 'db', 'sql', 'mysql', 'postgresql'],
            'financial_data': ['finance', 'trading', 'stock', 'market', 'transaction'],
            'healthcare_data': ['health', 'medical', 'patient', 'hospital'],
            'iot_data': ['iot', 'sensor', 'device', 'telemetry'],
            'social_data': ['social', 'user', 'profile', 'activity']
        }
        
        self.encoding_types = ['utf-8', 'utf-8-sig', 'latin1', 'cp1252', 'iso-8859-1', 'ascii']
        self.separator_types = [',', ';', '\t', '|', ' ', ':', '#']
        
    def detect_file_type(self, file_path):
        """íŒŒì¼ ê²½ë¡œì™€ ë‚´ìš©ìœ¼ë¡œë¶€í„° íŒŒì¼ ìœ í˜• ìë™ ê°ì§€"""
        file_name = Path(file_path).name.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ê°ì§€
        for file_type, keywords in self.supported_formats.items():
            if any(keyword in file_name for keyword in keywords):
                return file_type
        
        return 'generic_data'
    
    def detect_encoding(self, file_path, sample_size=10000):
        """íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€"""
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read(sample_size)
                result = chardet.detect(raw_data)
                return result.get('encoding', 'utf-8')
        except Exception:
            return 'utf-8'
    
    def detect_separator(self, file_path, encoding='utf-8'):
        """CSV êµ¬ë¶„ì ìë™ ê°ì§€"""
        try:
            with open(file_path, 'r', encoding=encoding, errors='ignore') as f:
                # ì²« 5ì¤„ë¡œ êµ¬ë¶„ì ê°ì§€
                sample = ''.join([f.readline() for _ in range(5)])
                
                for sep in self.separator_types:
                    if sample.count(sep) > sample.count(',') * 0.8:
                        return sep
                
                return ','  # ê¸°ë³¸ê°’
        except Exception:
            return ','
    
    def analyze_csv(self, file_path):
        """CSV íŒŒì¼ ë¶„ì„"""
        try:
            # íŒŒì¼ ì •ë³´ ìˆ˜ì§‘
            file_info = {
                'file_path': str(file_path),
                'file_name': Path(file_path).name,
                'file_size': os.path.getsize(file_path),
                'file_type': self.detect_file_type(file_path),
                'timestamp': datetime.now().isoformat()
            }
            
            # ì¸ì½”ë”© ë° êµ¬ë¶„ì ê°ì§€
            encoding = self.detect_encoding(file_path)
            separator = self.detect_separator(file_path, encoding)
            
            # ë°ì´í„° ë¡œë“œ
            df = pd.read_csv(file_path, encoding=encoding, sep=separator, low_memory=False)
            
            # ê¸°ë³¸ í†µê³„
            basic_stats = {
                'total_records': len(df),
                'total_columns': len(df.columns),
                'columns': list(df.columns),
                'data_types': df.dtypes.to_dict(),
                'memory_usage': df.memory_usage(deep=True).sum(),
                'null_counts': df.isnull().sum().to_dict(),
                'duplicate_rows': df.duplicated().sum()
            }
            
            # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
            quality_metrics = {
                'completeness': (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,
                'uniqueness': (1 - df.duplicated().sum() / len(df)) * 100,
                'consistency': self._analyze_consistency(df),
                'validity': self._analyze_validity(df)
            }
            
            # ì´ìƒ íƒì§€
            anomalies = self._detect_anomalies(df, file_info['file_type'])
            
            # íŒ¨í„´ ë¶„ì„
            patterns = self._analyze_patterns(df, file_info['file_type'])
            
            # ê²°ê³¼ í†µí•©
            analysis_result = {
                'file_info': file_info,
                'format_info': {
                    'encoding': encoding,
                    'separator': separator,
                    'detected_type': file_info['file_type']
                },
                'basic_stats': basic_stats,
                'quality_metrics': quality_metrics,
                'anomalies': anomalies,
                'patterns': patterns,
                'status': 'success'
            }
            
            return analysis_result
            
        except Exception as e:
            return {
                'file_info': {
                    'file_path': str(file_path),
                    'file_name': Path(file_path).name,
                    'file_size': os.path.getsize(file_path),
                    'timestamp': datetime.now().isoformat()
                },
                'error': str(e),
                'status': 'failed'
            }
    
    def _analyze_consistency(self, df):
        """ë°ì´í„° ì¼ê´€ì„± ë¶„ì„"""
        consistency_score = 100
        
        for col in df.columns:
            if df[col].dtype == 'object':
                # ë¬¸ìì—´ ì»¬ëŸ¼ì˜ ì¼ê´€ì„± ê²€ì‚¬
                unique_values = df[col].dropna().nunique()
                total_values = len(df[col].dropna())
                
                if total_values > 0:
                    consistency_ratio = unique_values / total_values
                    if consistency_ratio > 0.95:  # ë„ˆë¬´ ë§ì€ ê³ ìœ ê°’
                        consistency_score -= 10
                    elif consistency_ratio < 0.1:  # ë„ˆë¬´ ì ì€ ê³ ìœ ê°’
                        consistency_score -= 5
        
        return max(0, consistency_score)
    
    def _analyze_validity(self, df):
        """ë°ì´í„° ìœ íš¨ì„± ë¶„ì„"""
        validity_score = 100
        
        for col in df.columns:
            if df[col].dtype == 'object':
                # ë¹ˆ ë¬¸ìì—´ ì²´í¬
                empty_strings = (df[col] == '').sum()
                if empty_strings > 0:
                    validity_score -= (empty_strings / len(df)) * 20
                
                # íŠ¹ìˆ˜ ë¬¸ì ì²´í¬
                special_chars = df[col].str.contains(r'[^\w\s\-\.]', na=False).sum()
                if special_chars > len(df) * 0.1:
                    validity_score -= 5
        
        return max(0, validity_score)
    
    def _detect_anomalies(self, df, file_type):
        """ì´ìƒ íƒì§€"""
        anomalies = []
        
        # ê¸°ë³¸ ì´ìƒ íƒì§€
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                # ìˆ˜ì¹˜í˜• ë°ì´í„° ì´ìƒ íƒì§€
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                if len(outliers) > 0:
                    anomalies.append({
                        'type': 'numerical_outlier',
                        'column': col,
                        'count': len(outliers),
                        'percentage': (len(outliers) / len(df)) * 100
                    })
        
        # íŒŒì¼ ìœ í˜•ë³„ íŠ¹í™” ì´ìƒ íƒì§€
        if file_type == 'web_logs':
            anomalies.extend(self._detect_web_log_anomalies(df))
        elif file_type == 'financial_data':
            anomalies.extend(self._detect_financial_anomalies(df))
        
        return anomalies
    
    def _detect_web_log_anomalies(self, df):
        """ì›¹ ë¡œê·¸ íŠ¹í™” ì´ìƒ íƒì§€"""
        anomalies = []
        
        # IP ì£¼ì†Œ ì´ìƒ íƒì§€
        if 'ip' in df.columns.str.lower() or 'client_ip' in df.columns.str.lower():
            ip_col = 'ip' if 'ip' in df.columns.str.lower() else 'client_ip'
            ip_counts = df[ip_col].value_counts()
            suspicious_ips = ip_counts[ip_counts > ip_counts.mean() + 2 * ip_counts.std()]
            
            if len(suspicious_ips) > 0:
                anomalies.append({
                    'type': 'suspicious_ip',
                    'count': len(suspicious_ips),
                    'ips': suspicious_ips.head().to_dict()
                })
        
        # ìƒíƒœ ì½”ë“œ ì´ìƒ íƒì§€
        if 'status' in df.columns.str.lower():
            status_col = 'status'
            error_codes = df[df[status_col] >= 400]
            if len(error_codes) > 0:
                anomalies.append({
                    'type': 'high_error_rate',
                    'count': len(error_codes),
                    'percentage': (len(error_codes) / len(df)) * 100
                })
        
        return anomalies
    
    def _detect_financial_anomalies(self, df):
        """ê¸ˆìœµ ë°ì´í„° íŠ¹í™” ì´ìƒ íƒì§€"""
        anomalies = []
        
        # ê¸ˆì•¡ ê´€ë ¨ ì»¬ëŸ¼ ì´ìƒ íƒì§€
        amount_cols = [col for col in df.columns if any(keyword in col.lower() 
                      for keyword in ['amount', 'price', 'value', 'cost', 'fee'])]
        
        for col in amount_cols:
            if df[col].dtype in ['int64', 'float64']:
                # ìŒìˆ˜ ê¸ˆì•¡ íƒì§€
                negative_amounts = df[df[col] < 0]
                if len(negative_amounts) > 0:
                    anomalies.append({
                        'type': 'negative_amount',
                        'column': col,
                        'count': len(negative_amounts)
                    })
                
                # ë¹„ì •ìƒì ìœ¼ë¡œ í° ê¸ˆì•¡ íƒì§€
                large_amounts = df[df[col] > df[col].quantile(0.99) * 10]
                if len(large_amounts) > 0:
                    anomalies.append({
                        'type': 'unusually_large_amount',
                        'column': col,
                        'count': len(large_amounts)
                    })
        
        return anomalies
    
    def _analyze_patterns(self, df, file_type):
        """íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        # ì‹œê°„ íŒ¨í„´ ë¶„ì„
        time_cols = [col for col in df.columns if any(keyword in col.lower() 
                   for keyword in ['time', 'date', 'timestamp', 'created', 'updated'])]
        
        if time_cols:
            patterns['temporal'] = self._analyze_temporal_patterns(df, time_cols[0])
        
        # íŒŒì¼ ìœ í˜•ë³„ íŒ¨í„´ ë¶„ì„
        if file_type == 'web_logs':
            patterns['web'] = self._analyze_web_patterns(df)
        elif file_type == 'financial_data':
            patterns['financial'] = self._analyze_financial_patterns(df)
        
        return patterns
    
    def _analyze_temporal_patterns(self, df, time_col):
        """ì‹œê°„ íŒ¨í„´ ë¶„ì„"""
        try:
            df[time_col] = pd.to_datetime(df[time_col], errors='coerce')
            df_clean = df.dropna(subset=[time_col])
            
            if len(df_clean) == 0:
                return {}
            
            # ì‹œê°„ëŒ€ë³„ ë¶„í¬
            hourly_dist = df_clean[time_col].dt.hour.value_counts().sort_index()
            
            # ìš”ì¼ë³„ ë¶„í¬
            daily_dist = df_clean[time_col].dt.dayofweek.value_counts().sort_index()
            
            return {
                'hourly_distribution': hourly_dist.to_dict(),
                'daily_distribution': daily_dist.to_dict(),
                'time_span': {
                    'start': df_clean[time_col].min().isoformat(),
                    'end': df_clean[time_col].max().isoformat()
                }
            }
        except Exception:
            return {}
    
    def _analyze_web_patterns(self, df):
        """ì›¹ ë¡œê·¸ íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        # URL íŒ¨í„´ ë¶„ì„
        if 'url' in df.columns.str.lower() or 'path' in df.columns.str.lower():
            url_col = 'url' if 'url' in df.columns.str.lower() else 'path'
            url_patterns = df[url_col].value_counts().head(10)
            patterns['top_urls'] = url_patterns.to_dict()
        
        # User-Agent íŒ¨í„´ ë¶„ì„
        if 'user_agent' in df.columns.str.lower():
            ua_patterns = df['user_agent'].value_counts().head(10)
            patterns['top_user_agents'] = ua_patterns.to_dict()
        
        return patterns
    
    def _analyze_financial_patterns(self, df):
        """ê¸ˆìœµ ë°ì´í„° íŒ¨í„´ ë¶„ì„"""
        patterns = {}
        
        # ê±°ë˜ íŒ¨í„´ ë¶„ì„
        amount_cols = [col for col in df.columns if any(keyword in col.lower() 
                      for keyword in ['amount', 'price', 'value'])]
        
        if amount_cols:
            patterns['transaction_summary'] = {
                'total_transactions': len(df),
                'total_amount': df[amount_cols[0]].sum() if len(amount_cols) > 0 else 0,
                'average_amount': df[amount_cols[0]].mean() if len(amount_cols) > 0 else 0,
                'amount_range': {
                    'min': df[amount_cols[0]].min() if len(amount_cols) > 0 else 0,
                    'max': df[amount_cols[0]].max() if len(amount_cols) > 0 else 0
                }
            }
        
        return patterns
    
    def analyze_file(self, file_path):
        """íŒŒì¼ ë¶„ì„ ë©”ì¸ í•¨ìˆ˜"""
        print(f"ğŸ“Š {Path(file_path).name} ë¶„ì„ ì‹œì‘...")
        
        result = self.analyze_csv(file_path)
        
        if result['status'] == 'success':
            print("âœ… ë¶„ì„ ì™„ë£Œ!")
            print(f"   ğŸ“„ íŒŒì¼: {result['file_info']['file_name']}")
            print(f"   ğŸ“Š ë ˆì½”ë“œ: {result['basic_stats']['total_records']:,}ê°œ")
            print(f"   ğŸ“‹ ì»¬ëŸ¼: {result['basic_stats']['total_columns']}ê°œ")
            print(f"   ğŸ¯ íŒŒì¼ ìœ í˜•: {result['format_info']['detected_type']}")
            print(f"   ğŸ“ˆ ë°ì´í„° í’ˆì§ˆ: {result['quality_metrics']['completeness']:.1f}%")
            print(f"   âš ï¸  ì´ìƒ íƒì§€: {len(result['anomalies'])}ê°œ")
            
            return result
        else:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
            return None
    
    def generate_report(self, analysis_result, output_path=None):
        """ë¶„ì„ ê²°ê³¼ ë³´ê³ ì„œ ìƒì„±"""
        if not output_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"analysis_report_{timestamp}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ“„ ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_path}")
        return output_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Universal CSV Analyzer')
    parser.add_argument('file_path', help='ë¶„ì„í•  CSV íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--output', '-o', help='ê²°ê³¼ ì €ì¥ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    analyzer = UniversalCSVAnalyzer()
    result = analyzer.analyze_file(args.file_path)
    
    if result and args.output:
        analyzer.generate_report(result, args.output)

if __name__ == "__main__":
    main()
